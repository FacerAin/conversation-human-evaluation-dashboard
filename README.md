# conversation-human-evaluation-dashboard (For internal use)

A simple dashboard for human evaluation of conversations.

![demo](demo.png)

## Overview

This project provides a dashboard for evaluating the quality of conversations generated by different models. The dashboard allows users to score conversations based on various criteria such as consistency, engagingness, humanness, and memorability.

## Features

- Load and display conversation data from a JSONL file.
- Evaluate conversations using multiple models.
- Score conversations based on predefined criteria.
- Display historical and current conversation dialogues.

## Installation

To install the required dependencies, use [Poetry](https://python-poetry.org/):

```sh
poetry install
```

## Usage
To run the dashboard, execute the following command:

```sh
poetry run streamlit run app.py
```

## Configuration
The dashboard configuration is stored in the config.json file. The configuration includes the list of models, labelers, and the path to the data file.

Example config.json:
```sh
{
    "MODEL_LIST": ["model1", "model2", "model3"],
    "LABELER_LIST": ["labeler1", "labeler2"],
    "DATA_PATH": "dataset/human_eval_file.jsonl"
}
```
## Data Format
The data file should be in JSONL format, where each line represents a conversation session. Each session should include historical and current dialogues, as well as responses from different models.

Example human_eval_file.jsonl:
```
{
    "History": [
        {"utterance": "Hello, how can I help you today?"},
        {"utterance": "I need some information about your services."}
    ],
    "Current": [
        {"utterance": "Sure, what would you like to know?"},
        {"utterance": "Can you tell me about your pricing?"}
    ],
    "model1": "Our pricing starts at $10 per month.",
    "model2": "We offer competitive pricing starting at $10 per month.",
    "model3": "Our services are priced at $10 per month."
}
```
## License
TBA